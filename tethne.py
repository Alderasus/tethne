#
#   ISI Web of Science Triple Extraction Project 2012
#
#   Version:    0.2
#   Author:     Erick Peirson
#   Contact:    erick.peirson@asu.edu
#   
#   The primary objective of this project is to generate networks of scientific 
#   publications from the Web of Science database. Existing citation analysis 
#   software (such as CiteSpace II) is primarily oriented toward co-citation
#   analysis and visualization, and does not easily allow visualization and
#   analysis of citation relationships themselves. This script aims to fill
#   that void.
#

from optparse import OptionParser
import pickle
import datetime
import pprint

def contains(list, filter):
    for x in list:
        if filter(x):
            return True
    return False
    

class wos_object:   # -------------------------------------------------------- #
    #   > identifier should be a CR-like string: Last FM, 2012, J EXP BIOL, V210
    #   > authors should be a dictionary that maps CR-like to AF-like
    #   > citations should be a list of CR-like strings
    #   > meta should be a dictionary, containing anything
    def __init__(self, authors, pub_year, identifier, citations, meta):
        self.authors = authors
        self.pub_year = pub_year
        self.identifier = identifier
        self.citations = citations
        self.meta = meta
        # --------------------- END METHOD __init__ --------------------- #
# --------------------------- END CLASS wos_object --------------------------- #



class triple:   # ------------------------------------------------------------ #
    #   subject, predicate, and object are all strings.
    def __init__(self, subject, predicate, object):
        self.subject = subject
        self.predicate = predicate
        self.object = object
        # --------------------- END METHOD __init__ --------------------- #
# ----------------------------- END CLASS triple ----------------------------- #



class wos_library:   # ------------------------------------------------------- #
    def __init__(self, inputFile):
        self.data = inputFile
        self.library = []   #   This is the dictionary into which all    
                            #   wos_objects will be deposited
        self.relations = [] #   List of relation objects generated by
                            #   self.citationNetwork()
        self.internal = []  #   As self.relations, but only where both subject
                            #   and object are entries in the dataset.
        # --------------------- END METHOD __init__ --------------------- #                    
    
    def getEntry(self, filter): # -------------------------------------------- #
        for x in self.library:
            if filter(x):
                return x
        return False
        
    def buildLibrary(self):   # ---------------------------------------------- #
    #   Reads self.data file, and builds self.library. self.library is a list of
    #   wos_object objects. Key features include the generation of a CR-like 
    #   identifier for each record, 
    
        f = open(self.data, 'r')
        
        x = 0                       #   Index for self.library
        
        cache = {}                  #   Will re-use for each record
        
        has_citations = 'false'     #   So that we don't get an index error
                                    #   when building new wos_object
        
        for line in f:
            line = line.replace("\n","").replace("\r","")     
            
            if len(line) > 1:       #   Anything there?
                prefix = line[0:2]
                                    #   Two-letter field id from WoS
            else:
                prefix = 'XX'
            
            if prefix == 'EF':      #   End of file
                break
            
            if prefix == 'ER':      #   End of record (paper)
                x += 1              #   Get ready for new record (paper)
                
                #   Create CR-like identifier for record
                identifier = cache['AU'][0].replace(",","") + ", " + cache['PY'][0] + ", " + cache['SO'][0] #+ ", V" + cache.get('VL',['0'])[0]
                identifier = identifier.upper()

                #   Need to set this up to translate common synonyms
                identifier = identifier.replace("JOURNAL OF ECOLOGY", "J ECOL")
                
                #   To keep track of the number of authors
                num_authors = 0
                
                #   Create authors dictionary
                authors = {}
                for au in cache['AU']:
                    num_authors += 1
                    found = 'false'
                    au_last = au.split(',')[0]
                    for af in cache['AF']:
                        af_last = af.split(',')[0]
                        if au_last.upper() == af_last.upper():
                            authors[au] = af
                            found = 'true'
                    if found != 'true':           # Maybe there is no AF entry
                        authors[au] = au
        
                cache['num_authors'] = num_authors
        
                #   So that we don't get an index error when creating WoS object
                if has_citations == 'true':
                    citations = cache['CR']
                else:
                    citations = []
                    
                #   Create WoS object
                object = wos_object(authors, cache['PY'][0], identifier, citations, cache)
                self.library.append(object)
                
                #   Reset for next entry
                cache = {}                      #   Dump the old record's cache
                has_citations = 'false'
                
            else:           #   We're still in the middle of a record
            
                #   Want lines of substance only
                if prefix != 'XX' and prefix != '':
                
                    #   E.g. in CR lists
                    if prefix == '  ':
                        prefix = last_prefix    #   So that this will show up
                                                #   as an item in the list
                                                #   corresponding to this
                                                #   prefix.
                                                
                    #   in this case, assume it's a line in a new field
                    else:
                        #   Create a new prefix-rooted list
                        cache[prefix] = []  
                    if prefix != 'XX':
                        line_cache = line[3:].replace(".", "").upper() # Get value, remove periods, convert to uppercase
                        
                        #   Now look at citations
                        if prefix == 'CR':
                            has_citations = 'true'
                            
                            #   Only want author, year, journal/title
                            line_split = line_cache.split(",")
                            if len(line_split) > 3 :
                                line_cache = line_split[0] + "," + line_split[1] + "," + line_split[2]
                                #print line_cache
                                
                                # for unruly citations
                                #if line_split[3] == "":
                                #    line_split[3] = line_split[4]
                                    #line_split[4] = line_split[5]
                                
                                #if line_split[3][1] == 'V':
                                #    line_cache = line_cache + "," + line_split[3]

                        #   Add line to entry cache
                        cache[prefix].append(line_cache)
            last_prefix = prefix    #   In case we're still in a list of items
                                    #   in the same field (e.g. CR)
        print "WoS Library built successfully."
    # ---------------------- END METHOD buildLibrary --------------------- #
    
    def citationNetwork(self):   # ------------------------------------------- #
    #   Generates relations from identifier and CR items, and populates 
    #   self.relations.
    
        i = 0                       #   To count number of relations created
        self.relations = []         #   Replace existing relations database
                                    #   (could change this later to have a 
                                    #       flag -- append, replace, etc.)
        
        for entry in self.library:
            for citation in entry.citations:
                self.relations.append(triple(entry.identifier, "cites", citation))
                i = i + 1
        print i, "relations extracted."
    # -------------------- END METHOD citationNetwork -------------------- #

    def internalNetwork(self, parameters):
    #   Generates relations from identifier and CR items, but ONLY where both
    #   subject and object exist within as entries in the dataset.
    
        i = 0                       #   Count 'em
        self.internal = []          #   Start clean 
        
        print "Generating internal citation network..."
        
        #   Parameters set?
        try:
            parameters
        except NameError:
            parameters = None
        
        if len(self.relations) > 0:
            for triple in self.relations:
                #print triple.subject
                passesFilter = 'true'       #  
                if parameters is not None:
                    entry = self.getEntry(lambda x: x.identifier == triple.subject)
                    if not (parameters['startPY'] <= int(entry.meta['PY'][0]) <= parameters['endPY']):
                        passesFilter = 'false'
                if contains(self.library,lambda x: x.identifier == triple.object) and passesFilter == 'true':
                    self.internal.append(triple)
        else:
            print "Please run .citationNetwork() method before requesting internal network."

        print self.relations[1].object
        print self.library[1].identifier.replace("JOURNAL OF ECOLOGY", "J ECOL")

    def generateSIF(self, outputFile, scope):   # ---------------------------- #
    #   Generates SIF output file based on self.relations
    
        f = open(outputFile, "w")

        if scope == "internal":
            for relation in self.internal:
                sifLine = relation.subject.replace(" ","_") + " " + relation.predicate + " " + relation.object.replace(" ","_") + "\n"
                f.write(sifLine)        
        if scope == "global":
            for relation in self.relations:
                sifLine = relation.subject.replace(" ","_") + " " + relation.predicate + " " + relation.object.replace(" ","_") + "\n"
                f.write(sifLine)
    # ---------------------- END METHOD generateSIF ---------------------- #


    # This should eventually take cues about what attributes to include, and which nodes to include
    def generateSIFNodes(self, outputFile):
        f = open(outputFile, "w")
        f.write("pub_year" + "\n")
        
        for entry in self.library:
            f.write(entry.identifier.replace(" ","_") + " = " + entry.pub_year + "\n")


    def overlap(self,libObjA, libObjB):
    #   Return number of shared objects between libObjA and libObjB
        return list(set(libObjA.citations) & set(libObjB.citations))
    
    
    def compareAbsolute(self, start, end, threshold, path): #   path is output directory
    
#        path = "/Users/erickpeirson/Desktop/bibliographic_output/je/je_"
    

        histogram = {}
        sif = open(path+"relations.sif", "w")
        eda = open(path+"edges.eda", "w")
        eda.write("overlapAbsolute\n")
        
        noa = open(path+"nodes.noa", "w")
        noa.write("pub_year\n")
        
        noa_authors = open(path+"nodes_authors.noa", "w")
        noa_authors.write("authors\n")
        
        noa_title = open(path+"nodes_title.noa", "w")
        noa_title.write("title\n")
        
        noa_doi = open(path+"nodes_doi.noa", "w")
        noa_doi.write("doi\n")
        
        noa_rp = open(path+"nodes_rp.noa", "w")
        noa_rp.write("reprintAuthor\n")        

        x = start
        while x < end:
            include = "no"
            i = x
            while i < end:
                if (i != x):    # don't compare to self
                    overlap = self.overlap(self.library[x], self.library[i])     # This isn't quite right...
                    
                    
                    if str(len(overlap)) in histogram:
                        histogram[str(len(overlap))] += 1
                    else:
                        histogram[str(len(overlap))] = 1
                    if len(overlap) > threshold:
                        include = "yes" # flag to write node attributes to disk
                        
                        # Write edge and edge attributes to disk
                        sif.write(self.library[x].identifier.replace(" ","_") + " ovp " + self.library[i].identifier.replace(" ","_") + "\n")
                        eda.write(self.library[x].identifier.replace(" ","_") + " (ovp) " + self.library[i].identifier.replace(" ","_") + " = " + str(len(overlap)) + "\n")
                
                i = i + 1
                
            if include == "yes":
                # Write node attributes to disk
                noa.write(self.library[x].identifier.replace(" ","_") + " = " + self.library[x].pub_year + "\n")
                
                authors = ""
                for key in self.library[x].authors:
                    authors += self.library[x].authors[key] + "; "
                
                noa_authors.write(self.library[x].identifier.replace(" ","_") + " = " + authors + "\n")
                noa_title.write(self.library[x].identifier.replace(" ","_") + " = " + self.library[x].meta['TI'][0] + "\n")
                if self.library[x].meta.get('DI') != None:             # To avoid key error
                    noa_doi.write(self.library[x].identifier.replace(" ","_") + " = " + self.library[x].meta['DI'][0] + "\n")
                if self.library[x].meta.get('RP') != None:             # To avoid key error
                    noa_rp.write(self.library[x].identifier.replace(" ","_") + " = " + self.library[x].meta['RP'][0] + "\n")
            x = x + 1
        #n = 0
        #while n < (len(histogram) - 2):
        #    print str(n) + "    " + str(histogram[str(n)])
        #    n += 1


    def export(self, file, format, delim):
        print "Running export .... "
        f = open(file, "w")
    
        if format is "csv":
            # Headers
            f.write( "Identifier" + delim + "Title" + delim + "Authors" + delim + "WoS Identifier" + delim + "Journal" + delim + "Volume" + delim + "Page" + delim + "DOI" + delim + "Num Authors\n" )
            for entry in self.library:
                
                # Authors are separated by a colon -> : <-
                authors = ""
                for author in entry.meta['AU']:
                    authors += ":" + author
                authors = authors[1:]
                
                datum = entry.identifier + delim + entry.meta['TI'][0] + delim + authors + delim + entry.meta['UT'][0] + delim + entry.meta['SO'][0]
            
                if 'VL' in entry.meta:
                    datum += delim + entry.meta['VL'][0]
                    if 'BP' in entry.meta:
                        datum += delim + entry.meta['BP'][0]
                    else:
                        datum += delim
                else:
                    datum += delim + delim
                    
                if 'DI' in entry.meta:
                    datum += delim + entry.meta['DI'][0]
                else:
                    datum += delim
                
                datum += delim + str(entry.meta['num_authors'])

                f.write(datum + "\n")
            print "done.\n"
        f.close()
        return True
            
    def dump (self, file):
        f = open(file, 'wb')
        pickle.dump(self.library, f, -1)
        f.close()
        return True

    # This will load a pickled library from disk, and overwrite the current library
    def undump (self, file):
        f = open(file, 'rb')
        self.library = pickle.load(f)
        f.close()
        return True


    def authors_per_publication (self, file, start = 1900, end = datetime.datetime.now().year, slice = 1):
        print "Calculating authors per publication..."
        data = {}
        f = open(file, 'w')
        f.write("\t".join(["Year","Mean","Variance","Min","Max","N"]) + "\n")
        
        #   Populate fields for years
        for i in range (start, end):
            data[i] = {}
            data[i]['values'] = []
        
        #   Add num_authors values to list for each year
        for entry in self.library:
            if start < int(entry.pub_year) < end:
                data[int(entry.pub_year)]['values'].append(entry.meta['num_authors'])

        #   Calculate average, min, max, variance
        for datum in data:
        
            data[datum]['n'] = float(len(data[datum]['values']))
            if data[datum]['n'] > 0:
            
                #   Calculate the mean.
                sum = 0
                for v in data[datum]['values']:
                    sum += v
                data[datum]['mean'] = sum/data[datum]['n']

                #   Calculate the variance.
                sum_diff2 = 0
                for v in data[datum]['values']:
                    diff = v - data[datum]['mean']
                    diff2 = diff*diff
                    sum_diff2 += diff2
                data[datum]['variance'] = sum_diff2/data[datum]['n']

                data[datum]['min'] = min(data[datum]['values'])
                data[datum]['max'] = max(data[datum]['values'])

                f.write("\t".join([str(datum), str(data[datum]['mean']), str(data[datum]['variance']), str( data[datum]['min']), str(data[datum]['max']), str(data[datum]['n'])]) + "\n")
        f.close()
        print "done. \n"
        return True

# -------------------------- END CLASS wos_library --------------------------- #


# ---------------------------------- SANDBOX --------------------------------- #

parser = OptionParser()
parser.add_option("-d", "--data-path", dest="data_path")                        # String: Path to WoS data file
parser.add_option("-n", "--network-type", dest="network_type")                  # String: Type of network: 'bc' or 'dc'
parser.add_option("-t", "--overlap-threshold", dest="overlap_threshold", type="int")        # Int: Overlap threshold for bibliographic coupling
parser.add_option("-o", "--output-path", dest="output_path")                    # String: Path to output directory

(options, args) = parser.parse_args()

if (options.data_path != ""):
    if (options.output_path != "" ):
        # Initialize WoS Library
        library = wos_library(options.data_path)                                    #"./data/jecology/download_jEcol_1930-1979.txt"
        library.buildLibrary()
        library.dump(options.output_path + "cache.pickle")          # !! Need to go back and make sure a following slash is present
        
        if (options.output_path[-1] != "/"):
            options.output_path += "/"
        
        if (options.network_type == 'bc'):
            print "Generating bibliographic coupling network, with overlap threshold of " + str(options.overlap_threshold) + "."
            library.compareAbsolute(0, len(library.library),options.overlap_threshold,options.output_path)
            print "Bibliographic coupling network and attributes saved to " + options.output_path
        elif (options.network_type == 'dc'):
            print "Generating direct-citation network..."
            library.citationNetwork()
            library.internalNetwork({'startPY':0, 'endPY':2012})                        # This should be an option at some point
            library.generateSIF(options.output_path + "dc_internal.sif", "internal")    #"/Users/erickpeirson/Desktop/bibliographic_output/je/output_internal_0-2012.sif"
            library.generateSIFNodes(options.output_path + "dc_internal-pub_year.noa")
            print "Direct-citation network saved to " + options.output_path
        elif (options.network_type == 'csv'):
            library.export("./out.csv", "csv", "\t")
            print "Library exported to ./out.csv"
        elif (options.network_type == 'authors_per_publication'):
            library.export(options.output_path + "records.csv", "csv", "\t")
            library.authors_per_publication(options.output_path + "data.csv",1900,2013,1)
        else:
            print "No network type specified. Use --network-type option."
    else:
        print "No output directory specified. Use --output-path option."
else:
    print "No data file specified. Use --data-path option."