#
#   ISI Web of Science Triple Extraction Project 2012
#
#   Author:     Erick Peirson
#   Contact:    erick.peirson@asu.edu
#   
#   The primary objective of this project is to generate networks of scientific 
#   publications from the Web of Science database. Existing citation analysis 
#   software (such as CiteSpace II) is primarily oriented toward co-citation
#   analysis and visualization, and does not easily allow visualization and
#   analysis of citation relationships themselves. This script aims to fill
#   that void.
#

from optparse import OptionParser
import pickle
import datetime
from pprint import pprint
import gnosis

def contains(list, filter):
    for x in list:
        if filter(x):
            return True
    return False

class wos_object:
    #   > identifier should be a CR-like string: Last FM, 2012, J EXP BIOL, V210
    #   > authors should be a dictionary that maps CR-like to AF-like
    #   > citations should be a list of CR-like strings
    #   > meta should be a dictionary, containing anything
    def __init__(self, authors, pub_year, identifier, wosid, journal, doc_type, meta, title, citations = None):
        self.authors = authors
        self.year = pub_year
        self.identifier = identifier
        self.citations = citations
        self.wosid = wosid
        self.journal = journal
        self.doc_type = doc_type
        self.meta = meta
        self.title = title

class triple: 
#   subject, predicate, and object are all strings.
    def __init__(self, subject, predicate, object, attributes = {}):
        self.subject = subject
        self.predicate = predicate
        self.object = object
        self.attributes = attributes
        
class wos_author:
    def __init__ (self, short_name, full_name, first_pub_year=None, address=None):
        self.short_name = short_name
        self.full_name = full_name
        self.first_pub_year = first_pub_year
        self.address = address

class wos_library:
    def __init__(self, identifier, inputFile):
        self.identifier = identifier    #   So that we can track data
        self.data = inputFile
        self.library = []   #   This is the dictionary into which all wos_objects will be deposited
        self.relations = [] #   List of relation objects generated by self.citationNetwork()
        self.internal = []  #   As self.relations, but only where both subject and object are entries in the dataset.
        self.size = None    #   Number of records in the library. Set by buildLibrary()
                    
    def getEntry(self, filter):
        for x in self.library:
            if filter(x):
                return x
        return False
    
    def getAll(self, filter):
        results = []
        for x in self.library:
            if filter(x):
                results.append(x)
        return results
    
#   Reads self.data file, and builds self.library. self.library is a list of wos_object objects. Key features include the generation of a CR-like identifier for each record,        
    def buildLibrary(self):
        f = open(self.data, 'r')
        self.size = 0               #   To count the number of records.
        cache = {}                  #   Will re-use for each record
        for line in f:
            line = line.replace("\n","").replace("\r","")
            if len(line) > 1:
                prefix = line[0:2]  #   The WoS data file uses two-letter prefixes to indicate field types.
            else:
                prefix = 'XX'       #   So I can ignore useless lines later.
                
        #   At the end of the data file
            if prefix == 'EF':
                break
                
        #   At the end of a record (paper)
            if prefix == 'ER':
                self.size += 1
                
            #   Create CR-like identifier for record
                identifier = cache['AU'][0].replace(",","") + ", " + cache['PY'][0] + ", " + cache['SO'][0] #+ ", V" + cache.get('VL',['0'])[0]
                identifier = identifier.upper()
                
            #   TODO: At some point need to implement an alias list, or something.
                identifier = identifier.replace("JOURNAL OF ECOLOGY", "J ECOL")
                
            #   TODO: Need to figure out why I did this....
                num_authors = 0
                authors = {}
                for au in cache['AU']:
                    num_authors += 1
                    found = 'false'
                    au_last = au.split(',')[0]
                    for af in cache['AF']:
                        af_last = af.split(',')[0]
                        if au_last.upper() == af_last.upper():
                            authors[au] = af
                            found = 'true'
                    if found != 'true':           # Maybe there is no AF entry
                        authors[au] = au
                cache['num_authors'] = num_authors
        
            #   Some titles bleed over into multiple rows
                title = ''
                for row in cache['TI']:
                    title += row
        
                self.library.append(wos_object(authors, int(cache['PY'][0]), identifier, cache['UT'][0], cache['SO'][0], cache['DT'][0], cache, title, cache['CR']))

            #   Dump for next record
                cache = {}
                cache['CR'] = None

        #   We're still in the middle of a record...
            else:           
                if prefix != 'XX' and prefix != '':
                    if prefix == '  ':          #   If there is no prefix, the line is part of the field to which the previous line belonged.
                        prefix = last_prefix
                    else:                       #   If there is a prefix, the line is part of the next field.
                        cache[prefix] = []  
                    if prefix != 'XX':
                        line_cache = line[3:].replace(".", "").upper()
                        
                    #   CR contains all of the paper's references
                        if prefix == 'CR':
                        #   Only want author, year, journal/title, which are the first three comma-separated values
                            line_split = line_cache.split(",")
                            if len(line_split) > 3 :
                                line_cache = line_split[0] + "," + line_split[1] + "," + line_split[2]
                        
                        cache[prefix].append(line_cache)
            last_prefix = prefix
        return None
        
#   Direct citations
    def citationNetwork(self):
    #   Generates relations from identifier and CR items, and populates self.relations.
        i = 0                       #   To count number of relations created
        self.relations = []         #   TODO: Could change this later to have a flag -- append, replace, etc.
        for entry in self.library:
            if entry.citations is not None:
                for citation in entry.citations:
                    self.relations.append(triple(entry.identifier, "cites", citation))
                    i = i + 1
        return None

    def coauthors(self, graph_type):
        self.authors = []
        self.author_graph = []
        self.author_papers = {}
        self.coauthor_graph = []
        self.papers = []
        self.papers_data = {}
        self.author_paper_xgmml = ''
        self.coauthor_xgmml = ''
        
        for entry in self.library:
            self.papers.append(entry.wosid)
            p_index = self.papers.index(entry.wosid)
            self.papers_data[entry.wosid] = entry
            for author in entry.meta['AU']:
                try:
                    a_index = self.authors.index(author)
                except ValueError:
                    self.authors.append(author)
                    a_index = self.authors.index(author)
                    self.author_papers[a_index] = []
                self.author_papers[a_index].append((entry.wosid, entry.year))
                self.author_graph.append((str(a_index), str(p_index)))
                
        #   Now all authors are indexed
            for a in range(0, len(entry.meta['AU'])):
                for b in range(a, len(entry.meta['AU'])):
                    if a is not b:
                        self.coauthor_graph.append((str(self.authors.index(entry.meta['AU'][a])), str(self.authors.index(entry.meta['AU'][b])), entry.year))
                
    #   Author-paper XGMML dynamic network; assigned to self.author_paper_xgmml
        xgmml = '<?xml version="1.0" encoding="UTF-8" standalone="yes"?>'
        xgmml += '\n<graph label="Davidson Authors Papers" directed="1" id="5" start="1950" end="2013">'
        for author in self.authors:
            start_year = 2013
            for p in self.author_papers[self.authors.index(author)]:
                if int(p[1]) < int(start_year):
                    start_year = int(p[1])
            xgmml += '\n\t<node label="{0}" id="a_{1}" start="{2}">'.format(author, self.authors.index(author), start_year)
            xgmml += '\n\t\t<att name="type" type="string" value="author" />'
            xgmml += '\n\t\t<graphics fill="#800000" />'
            xgmml += '\n\t</node>'
        for paper in self.papers:
            xgmml += '\n\t<node label="{0}" id="p_{1}" start="{2}">'.format(paper, self.papers.index(paper), self.papers_data[paper].year)
            xgmml += '\n\t\t<graphics fill="#003366" />'
            xgmml += '\n\t\t<att name="type" type="string" value="paper" />'
            xgmml += '\n\t\t<att name="year" type="integer" value="{0}" />'.format(self.papers_data[paper].year)
            xgmml += '\n\t\t<att name="journal" type="string" value="{0}" />'.format(self.papers_data[paper].journal)
            xgmml += '\n\t\t<att name="doc_type" type="string" value="{0}" />'.format(self.papers_data[paper].doc_type)
            xgmml += '\n\t\t<att name="title" type="string" value="{0}" />'.format(self.papers_data[paper].title.replace('"', '\''))
            xgmml += '\n\t</node>'
        for edge in self.author_graph:
            xgmml += '\n\t<edge source="a_{0}" target="p_{1}" />'.format(edge[0], edge[1])
        xgmml += '\n</graph>'
        self.author_paper_xgmml = xgmml
    
    #   Coathor XGMML dynamic network; assigned to self.coauthor_xgmml
        xgmml = '<?xml version="1.0" encoding="UTF-8" standalone="yes"?>'
        xgmml += '\n<graph label="Davidson Co-Authors" directed="1" id="5" start="1950" end="2013">'
        for author in self.authors:
            start_year = 2013
            for p in self.author_papers[self.authors.index(author)]:
                if int(p[1]) < int(start_year):
                    start_year = int(p[1])
            xgmml += '\n\t<node label="{0}" id="a_{1}" start="{2}">'.format(author, self.authors.index(author), start_year)
            xgmml += '\n\t\t<att name="type" type="string" value="author" />'
            xgmml += '\n\t\t<graphics fill="#800000" />'
            xgmml += '\n\t</node>'
        for edge in self.coauthor_graph:
            xgmml += '\n\t<edge source="a_{0}" target="a_{1}" />'.format(edge[0], edge[1])
        xgmml += '\n</graph>'            
        self.coauthor_xgmml = xgmml

        return None

    def internalNetwork(self, parameters = None):
    #   Generates relations from identifier and CR items, but ONLY where both subject and object exist within as entries in the dataset.
        i = 0                       #   Count 'em
        self.internal = []          #   Start clean 

        if len(self.relations) > 0:     #   Must run buildLibrary() and citationNetwork()
            for triple in self.relations:
                passesFilter = 'true'
                if parameters is not None:
                    entry = self.getEntry(lambda x: x.identifier == triple.subject)
                    if not (parameters['startPY'] <= int(entry.meta['PY'][0]) <= parameters['endPY']):
                        passesFilter = 'false'
                if contains(self.library,lambda x: x.identifier == triple.object) and passesFilter == 'true':
                    self.internal.append(triple)
        else:
            print "Please run .citationNetwork() method before requesting internal network."    #   TODO turn this into an exception
            
        return None

    def generateSIF(self, outputFile, scope):
    #   Generates SIF output file based on self.relations
        f = open(outputFile, "w")
        if scope == "internal":
            for relation in self.internal:
                sifLine = relation.subject.replace(" ","_") + " " + relation.predicate + " " + relation.object.replace(" ","_") + "\n"
                f.write(sifLine)        
        if scope == "global":
            for relation in self.relations:
                sifLine = relation.subject.replace(" ","_") + " " + relation.predicate + " " + relation.object.replace(" ","_") + "\n"
                f.write(sifLine)
        f.close()
        return None

#   This should eventually take cues about what attributes to include, and which nodes to include
    def generateSIFNodes(self, outputFile):
        f = open(outputFile, "w")
        f.write("pub_year" + "\n")
        for entry in self.library:
            f.write(entry.identifier.replace(" ","_") + " = " + entry.year + "\n")
        f.close()
        return None

#   Return number of shared objects between libObjA and libObjB; (e.g. references shared by two papers)
    def overlap(self,libObjA, libObjB):
        return list(set(libObjA.citations) & set(libObjB.citations))
        
    def compareAbsolute(self, threshold, path, start_year, end_year): #   path is output directory
        histogram = {}
        out = open(path+"couplings.xgmml", "w")
        
        subset = self.getAll(lambda x: start_year < int(x.year) < end_year)

        couplings = []  #   Will be a list of triple objects
        nodes = []

        for x in range(0, len(subset)):
            node_added = 0
            for i in range(x, len(subset)):     #   Start at x to avoid doing each comparison twice.
                if (i != x):    #   Don't compare to self
                    overlap = self.overlap(subset[x], subset[i])     # This isn't quite right...
                    if len(overlap) > threshold:
                    #   This will add nodes for all paper in the library that fall within the specified time domain and have a coupling relationship above threshold.
                        if node_added is 0:
                            nodes.append (subset[x])
                            node_added = 1
                        couplings.append(triple(subset[x].wosid, "bc", subset[i].wosid, {'overlap': str(len(overlap)), 'start': max(subset[i].year,subset[x].year)}))
        
    #   XGMML dynamic network
        xgmml = '<?xml version="1.0" encoding="UTF-8" standalone="yes"?>'
        xgmml += '\n<graph label="asdf" directed="0" id="5" start="0" end="1.5">'
        for node in nodes:
            start = (float(node.year) - float(start_year))/(float(end_year) - float(start_year))
            xgmml += '\n\t<node label="{0}" id="{1}" start="{2}">'.format(node.identifier.replace("&","&amp;"), node.wosid, start)
            xgmml += '\n\t\t<att name="pub_year" type="integer" value="{0}" />'.format(node.year)
            xgmml += '\n\t\t<att name="journal" type="string" value="{0}" />'.format(node.journal.replace("&","&amp;"))
            xgmml += '\n\t\t<att name="num_authors" type="integer" value="{0}" />'.format(len(node.authors))
            xgmml += '\n\t</node>'
        
        for coupling in couplings:
            start = (float(coupling.attributes['start']) - float(start_year))/(float(end_year) - float(start_year))
            xgmml += '\n\t<edge label="coupling_{0}_{1}" source="{0}" target="{1}">'.format(coupling.subject, coupling.object)
            xgmml += '\n\t\t<att name="overlap" type="real" value="{0}" />'.format(str(1/float(coupling.attributes['overlap'])))
            xgmml += '\n\t</edge>'
        
        xgmml += '\n</graph>'
        
        out.write(xgmml)
        out.close()

    def export(self, file, format, delim):
        f = open(file, "w")
        if format is "csv":
            # Headers
            f.write( "Identifier" + delim + "Title" + delim + "Authors" + delim + "WoS Identifier" + delim + "Journal" + delim + "Volume" + delim + "Page" + delim + "DOI" + delim + "Num Authors\n" )
            for entry in self.library:
                # Authors are separated by a colon -> : <-
                authors = ""
                for author in entry.meta['AU']:
                    authors += ":" + author
                authors = authors[1:]
                datum = entry.identifier + delim + entry.meta['TI'][0] + delim + authors + delim + entry.wosid + delim + entry.meta['SO'][0]
                if 'VL' in entry.meta:
                    datum += delim + entry.meta['VL'][0]
                    if 'BP' in entry.meta:
                        datum += delim + entry.meta['BP'][0]
                    else:
                        datum += delim
                else:
                    datum += delim + delim
                if 'DI' in entry.meta:
                    datum += delim + entry.meta['DI'][0]
                else:
                    datum += delim
                datum += delim + str(entry.meta['num_authors'])
                f.write(datum + "\n")
        f.close()
        return None
            
    def dump (self, file):
        f = open(file, 'wb')
        pickle.dump(self.library, f, -1)
        f.close()
        return True

#   This will load a pickled library from disk, and overwrite the current library
    def undump (self, file):
        f = open(file, 'rb')
        self.library = pickle.load(f)
        f.close()
        return True
    
#   For Wes
    def authors_per_publication (self, file, start = 1900, end = datetime.datetime.now().year, slice = 1):
        data = {}
    #   TODO: should just return the data, rather than write it to file. Or have another method to do file-writing. Or something.
        f = open(file, 'w')
        f.write("\t".join(["Year","Mean","Variance","Min","Max","N"]) + "\n")
        
    #   Populate fields for years
        for i in range (start, end):
            data[i] = {}
            data[i]['values'] = []
        
    #   Add num_authors values to list for each year
        for entry in self.library:
            if start <= int(entry.year) <= end:
                data[int(entry.year)]['values'].append(entry.meta['num_authors'])

    #   Calculate average, min, max, variance
        for datum in data:
            data[datum]['n'] = float(len(data[datum]['values']))
            if data[datum]['n'] > 0:

            #   Calculate the mean.
                sum = 0
                for v in data[datum]['values']:
                    sum += v
                data[datum]['mean'] = sum/data[datum]['n']
            
            #   Calculate the variance.
                sum_diff2 = 0
                for v in data[datum]['values']:
                    diff = v - data[datum]['mean']
                    diff2 = diff*diff
                    sum_diff2 += diff2
                data[datum]['variance'] = sum_diff2/data[datum]['n']
            
            #   Min and Max
                data[datum]['min'] = min(data[datum]['values'])
                data[datum]['max'] = max(data[datum]['values'])

                f.write("\t".join([str(datum), str(data[datum]['mean']), str(data[datum]['variance']), str( data[datum]['min']), str(data[datum]['max']), str(data[datum]['n'])]) + "\n")
        f.close()
        return None